{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc628a5-c7a5-4510-9924-b6db9a52c05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosaNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from librosa) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from librosa) (0.59.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp312-abi3-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from librosa) (4.11.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from lazy-loader>=0.1->librosa) (23.2)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
      "Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "   ---------------------------------------- 0.0/260.1 kB ? eta -:--:--\n",
      "   ---------------------------------------  256.0/260.1 kB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 260.1/260.1 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.6/64.6 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 0.6/1.0 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.0/1.0 MB 16.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 12.9 MB/s eta 0:00:00\n",
      "Downloading soxr-0.5.0.post1-cp312-abi3-win_amd64.whl (164 kB)\n",
      "   ---------------------------------------- 0.0/164.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 164.9/164.9 kB 9.7 MB/s eta 0:00:00\n",
      "Installing collected packages: soxr, audioread, soundfile, pooch, librosa\n",
      "Successfully installed audioread-3.0.1 librosa-0.10.2.post1 pooch-1.8.2 soundfile-0.13.1 soxr-0.5.0.post1\n"
     ]
    }
   ],
   "source": [
    "pip install librosa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f8016d-16ff-446c-b600-d278afd2dc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Downloading sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from sounddevice) (1.16.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Downloading sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "   ---------------------------------------- 0.0/363.6 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 30.7/363.6 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  358.4/363.6 kB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 363.6/363.6 kB 3.2 MB/s eta 0:00:00\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5e9ca12-57ea-4d2b-839f-462313faca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "815e5a76-fce0-47be-8466-6a71df003b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
    "    return np.mean(mfccs.T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99c0c734-514d-4822-bc26-9732856daa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of loading and processing a dataset\n",
    "def load_dataset(dataset_path):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for emotion in os.listdir(dataset_path):\n",
    "        for file in os.listdir(os.path.join(dataset_path, emotion)):\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(dataset_path, emotion, file)\n",
    "                mfccs = extract_features(file_path)\n",
    "                features.append(mfccs)\n",
    "                labels.append(emotion)\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3773080d-9cf6-45ed-b08e-cf1e5427ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "871011c3-cc6f-4cf6-9a38-90ed75f6919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "features, labels = load_dataset('Voice_Emotion_Detection_Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e54a4444-5d7b-45a5-b96f-b3290d59f7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20cdf254-16b6-4d3b-a607-de65e1595d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9c24a06-6d44-4570-98e3-4aa4d046ef45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mihir\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.2531 - loss: 16.3762 - val_accuracy: 0.4393 - val_loss: 1.6247\n",
      "Epoch 2/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6080 - loss: 1.1346 - val_accuracy: 0.7393 - val_loss: 0.7366\n",
      "Epoch 3/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7970 - loss: 0.5792 - val_accuracy: 0.7893 - val_loss: 0.5645\n",
      "Epoch 4/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8808 - loss: 0.3987 - val_accuracy: 0.8321 - val_loss: 0.4899\n",
      "Epoch 5/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8849 - loss: 0.3650 - val_accuracy: 0.8393 - val_loss: 0.4787\n",
      "Epoch 6/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8859 - loss: 0.3742 - val_accuracy: 0.8821 - val_loss: 0.3646\n",
      "Epoch 7/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9179 - loss: 0.2646 - val_accuracy: 0.8750 - val_loss: 0.3640\n",
      "Epoch 8/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9248 - loss: 0.2425 - val_accuracy: 0.8679 - val_loss: 0.3496\n",
      "Epoch 9/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9301 - loss: 0.2293 - val_accuracy: 0.8750 - val_loss: 0.3868\n",
      "Epoch 10/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9354 - loss: 0.2233 - val_accuracy: 0.8929 - val_loss: 0.3563\n",
      "Epoch 11/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9269 - loss: 0.2359 - val_accuracy: 0.9179 - val_loss: 0.2457\n",
      "Epoch 12/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9157 - loss: 0.2426 - val_accuracy: 0.9071 - val_loss: 0.2835\n",
      "Epoch 13/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9231 - loss: 0.2428 - val_accuracy: 0.9143 - val_loss: 0.2500\n",
      "Epoch 14/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9388 - loss: 0.1680 - val_accuracy: 0.8714 - val_loss: 0.3447\n",
      "Epoch 15/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9076 - loss: 0.2390 - val_accuracy: 0.9179 - val_loss: 0.2639\n",
      "Epoch 16/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9353 - loss: 0.1885 - val_accuracy: 0.9107 - val_loss: 0.2751\n",
      "Epoch 17/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9174 - loss: 0.2381 - val_accuracy: 0.8643 - val_loss: 0.4095\n",
      "Epoch 18/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9311 - loss: 0.1929 - val_accuracy: 0.9071 - val_loss: 0.2819\n",
      "Epoch 19/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9227 - loss: 0.1901 - val_accuracy: 0.9107 - val_loss: 0.2316\n",
      "Epoch 20/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9392 - loss: 0.2028 - val_accuracy: 0.9464 - val_loss: 0.1779\n",
      "Epoch 21/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9565 - loss: 0.1221 - val_accuracy: 0.9071 - val_loss: 0.2694\n",
      "Epoch 22/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9549 - loss: 0.1459 - val_accuracy: 0.9071 - val_loss: 0.2280\n",
      "Epoch 23/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9500 - loss: 0.1416 - val_accuracy: 0.9107 - val_loss: 0.2749\n",
      "Epoch 24/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9393 - loss: 0.1756 - val_accuracy: 0.9464 - val_loss: 0.1750\n",
      "Epoch 25/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9582 - loss: 0.1162 - val_accuracy: 0.9357 - val_loss: 0.1950\n",
      "Epoch 26/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9580 - loss: 0.1110 - val_accuracy: 0.9464 - val_loss: 0.1677\n",
      "Epoch 27/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9572 - loss: 0.1341 - val_accuracy: 0.9393 - val_loss: 0.2031\n",
      "Epoch 28/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9417 - loss: 0.1504 - val_accuracy: 0.9429 - val_loss: 0.1750\n",
      "Epoch 29/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9433 - loss: 0.1549 - val_accuracy: 0.9464 - val_loss: 0.1636\n",
      "Epoch 30/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9604 - loss: 0.1282 - val_accuracy: 0.9143 - val_loss: 0.2664\n",
      "Epoch 31/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9531 - loss: 0.1456 - val_accuracy: 0.9357 - val_loss: 0.1832\n",
      "Epoch 32/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9650 - loss: 0.1019 - val_accuracy: 0.9464 - val_loss: 0.1816\n",
      "Epoch 33/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9558 - loss: 0.1247 - val_accuracy: 0.9464 - val_loss: 0.1706\n",
      "Epoch 34/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9659 - loss: 0.0954 - val_accuracy: 0.9464 - val_loss: 0.1663\n",
      "Epoch 35/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9644 - loss: 0.1098 - val_accuracy: 0.9250 - val_loss: 0.2193\n",
      "Epoch 36/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9316 - loss: 0.1867 - val_accuracy: 0.9214 - val_loss: 0.2314\n",
      "Epoch 37/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9475 - loss: 0.1395 - val_accuracy: 0.9321 - val_loss: 0.2015\n",
      "Epoch 38/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9375 - loss: 0.1548 - val_accuracy: 0.9214 - val_loss: 0.2297\n",
      "Epoch 39/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9527 - loss: 0.1338 - val_accuracy: 0.9107 - val_loss: 0.2711\n",
      "Epoch 40/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9329 - loss: 0.1827 - val_accuracy: 0.9429 - val_loss: 0.1799\n",
      "Epoch 41/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9552 - loss: 0.1227 - val_accuracy: 0.9321 - val_loss: 0.1855\n",
      "Epoch 42/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9609 - loss: 0.1134 - val_accuracy: 0.8929 - val_loss: 0.2829\n",
      "Epoch 43/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9353 - loss: 0.1681 - val_accuracy: 0.9357 - val_loss: 0.2289\n",
      "Epoch 44/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9636 - loss: 0.1036 - val_accuracy: 0.9321 - val_loss: 0.2086\n",
      "Epoch 45/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9680 - loss: 0.1060 - val_accuracy: 0.9357 - val_loss: 0.1931\n",
      "Epoch 46/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9605 - loss: 0.1146 - val_accuracy: 0.9214 - val_loss: 0.3002\n",
      "Epoch 47/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9432 - loss: 0.1858 - val_accuracy: 0.9071 - val_loss: 0.3340\n",
      "Epoch 48/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9445 - loss: 0.1734 - val_accuracy: 0.9321 - val_loss: 0.2399\n",
      "Epoch 49/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9549 - loss: 0.1379 - val_accuracy: 0.8929 - val_loss: 0.3725\n",
      "Epoch 50/50\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9428 - loss: 0.1905 - val_accuracy: 0.9321 - val_loss: 0.2099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ef867bbb30>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(len(np.unique(labels_encoded)), activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1dcf4570-7e62-451b-9188-ad4e9836b791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wavio in c:\\users\\mihir\\anaconda3\\lib\\site-packages (0.0.9)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\mihir\\anaconda3\\lib\\site-packages (from wavio) (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wavio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2ca1573-d6ce-40b2-aa02-4525f3d0d316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import sounddevice as sd\n",
    "import wavio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "899c795d-abe1-407a-835f-9d817ef2ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionDetectorApp:\n",
    "    def __init__(self, master):\n",
    "        self.master = master\n",
    "        master.title(\"Emotion Detection through Voice\")\n",
    "\n",
    "        self.label = tk.Label(master, text=\"Upload a voice note or record your voice:\")\n",
    "        self.label.pack()\n",
    "\n",
    "        self.upload_button = tk.Button(master, text=\"Upload Voice Note\", command=self.upload_file)\n",
    "        self.upload_button.pack()\n",
    "\n",
    "        self.record_button = tk.Button(master, text=\"Record Voice\", command=self.record_voice)\n",
    "        self.record_button.pack()\n",
    "\n",
    "    def upload_file(self):\n",
    "        file_path = filedialog.askopenfilename()\n",
    "        if file_path:\n",
    "            self.process_audio(file_path)\n",
    "\n",
    "    def record_voice(self):\n",
    "        fs = 44100  # Sample rate\n",
    "        seconds = 5  # Duration of recording\n",
    "        recording = sd.rec(int(seconds * fs), samplerate=fs, channels=1)\n",
    "        sd.wait()  # Wait until recording is finished\n",
    "        wavio.write(\"recording.wav\", recording, fs, sampwidth=2)\n",
    "        self.process_audio(\"recording.wav\")\n",
    "\n",
    "    def process_audio(self, file_path):\n",
    "        # Check if the voice is female (this is a placeholder, implement your own logic)\n",
    "        if not self.is_female_voice(file_path):\n",
    "            messagebox.showerror(\" Error\", \"Please upload a female voice.\")\n",
    "            return\n",
    "        # Extract features and make predictions\n",
    "        features = extract_features(file_path)\n",
    "        prediction = model.predict(np.array([features]))\n",
    "        emotion = le.inverse_transform([np.argmax(prediction)])\n",
    "        messagebox.showinfo(\"Detected Emotion\", f\"The detected emotion is: {emotion[0]}\")\n",
    "\n",
    "    def is_female_voice(self, file_path):\n",
    "        # Placeholder for actual voice gender detection logic\n",
    "        return True  # Assume all uploaded voices are female for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e1b081a-c372-4d2b-8910-60e20d2b1b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 569ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
     ]
    }
   ],
   "source": [
    "root = tk.Tk()\n",
    "app = EmotionDetectorApp(root)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94f08927-852a-4277-82d5-88e7b3f751e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "             Angry       0.91      0.86      0.88        35\n",
      "           Disgust       0.89      1.00      0.94        42\n",
      "              Fear       0.88      0.90      0.89        42\n",
      "             Happy       0.98      0.98      0.98        43\n",
      "           Neutral       0.92      0.96      0.94        46\n",
      "Pleasant_Surprised       0.97      0.90      0.94        40\n",
      "               Sad       1.00      0.91      0.95        32\n",
      "\n",
      "          accuracy                           0.93       280\n",
      "         macro avg       0.94      0.93      0.93       280\n",
      "      weighted avg       0.93      0.93      0.93       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
